---
title: "HW3 Peer Assessment"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Background

The fishing industry uses numerous measurements to describe a specific fish. Our goal is to predict the weight of a fish based on a number of these measurements and determine if any of these measurements are insignificant in determining the weigh of a product. See below for the description of these measurments.

## Data Description

The data consists of the following variables:

1.  **Weight**: weight of fish in g (numerical)
2.  **Species**: species name of fish (categorical)
3.  **Body.Height**: height of body of fish in cm (numerical)
4.  **Total.Length**: length of fish from mouth to tail in cm (numerical)
5.  **Diagonal.Length**: length of diagonal of main body of fish in cm (numerical)
6.  **Height**: height of head of fish in cm (numerical)
7.  **Width**: width of head of fish in cm (numerical)

## Read the data

```{r}
rm(list=ls())
setwd("E:/Data")
# Import library you may need
library(car)
# Read the data set
fishfull = read.csv("Fish.csv",header=T, fileEncoding = 'UTF-8-BOM')
row.cnt = nrow(fishfull)
# Split the data into training and testing sets
fishtest = fishfull[(row.cnt-9):row.cnt,]
fish = fishfull[1:(row.cnt-10),]

head(fish)
```

*Please use fish as your data set for the following questions unless otherwise stated.*

# Question 1: Exploratory Data Analysis [8 points]

**(a) Create a box plot comparing the response variable, *Weight*, across the multiple *species*. Based on this box plot, does there appear to be a relationship between the predictor and the response?**

```{r}
attach(fish)
boxplot(Weight ~ Species)

```

**Answer:**

There appears to be a relationship between Species and Weight. In the chart above, we can see that Parkki, Roach and Smelt have relatively lower average Weight (also smaller variances), while Bream, Pike and Whitefish have relatively higher average weight (also larger variances). However, we need further investigations to figure out if the this difference in average weight and varaince is statistically significant.

<br><br>

**(b) Create scatterplots of the response, *Weight*, against each quantitative predictor, namely** Body.Height**,** Total.Length**,** Diagonal.Length**,** Height**, and** Width**. Describe the general trend of each plot. Are there any potential outliers?**

```{r}

par(mfrow = c(2,3))

plot(Body.Height, Weight)
abline(lm(Weight ~ Body.Height), col = "red")

plot(Total.Length, Weight)
abline(lm(Weight ~ Total.Length), col = "red")

plot(Diagonal.Length, Weight)
abline(lm(Weight ~ Diagonal.Length), col = "red")

plot(Height, Weight)
abline(lm(Weight ~ Height), col = "red")

plot(Width, Weight)
abline(lm(Weight ~ Width), col = "red")
```

**Answer:**

The general trends of the relationship between Weight and each predictor are all positive. However, the relationship between Weight and each predictor is not exactly linear (needs further investigation), since there are some curvatures in each of the plot above.

In the scatter plots of Height and Width, we can observe some patterns of non-constant variance, since the variance of Weight gets larger as Height / Width increases.

We can also observe some potential outliers in more than one scatter plot when Weight is larger than 1500.

<br><br>

**(c) Display the correlations between each of the quantitative variables. Interpret the correlations in the context of the relationships of the predictors to the response and in the context of multicollinearity.**

```{r}

round(cor(fish[,-2]),3)
```

**Answer:**

From the results above, we can see that Weight has strong linear relationship with Body Height, Total Length, Diagonal Length and Width (correlation \> 0.7). It also has a moderate linear relationship with Height (correlation = 0.688). This observation indicates that a linear regression model should be appropriate.

However, the correlation coefficients of the following pairs of predictors are larger than 0.7 , indicating strong linear relationships. Therefore, the issue of multicolinearity may exist between these predictors:

Total. Length & Body.Height (correlation = 1)

Diagonal.Length & Body.Height (0.992)

Diagonal.Length & Total.Length (0.994)

Height & Diagonal.Length (0.705)

Width & Body.Height (0.866)

Width & Total Length (0.873)

Width & Diagonal.Length (0.877)

Width & Height (0.791)

<br><br>

**(d) Based on this exploratory analysis, is it reasonable to assume a multiple linear regression model for the relationship between *Weight* and the predictor variables?**

**Answer:**

It is reasonable to assume a multiple linear regression model. From the exploratory analysis above, we can observe a linear relationship (high correlation coefficients) between Weight and the predictor variables.

However, some variable selection methods need to be adopted when we build the model, in order to avoid the issue of multicolinearity.

<br><br>

# Question 2: Fitting the Multiple Linear Regression Model [8 points]

*Create the full model without transforming the response variable or predicting variables using the fish data set. Do not use fishtest*

**(a) Build a multiple linear regression model, called model1, using the response and all predictors. Display the summary table of the model.**

```{r}

model1 <- lm(Weight ~ Species + Body.Height + Total.Length + Diagonal.Length +Height + Width, data = fish)

summary(model1)
```

**(b) Is the overall regression significant at an** $\alpha$ level of 0.01? Explain.

**Answer:**

The p-value for the F test equals 2.2e-16, which is smaller than 0.01 . Therefore, the overall regression is significant at an $\alpha$ level of 0.01 .

<br><br>

**(c) What is the coefficient estimate for *Body.Height*? Interpret this coefficient.**

**Answer:**

The coefficient estimate of Body.Height equals -176.87 .

This means that when Body.Height is increased by 1 cm, Weight will decrease by 176.87 g , holding all other variables constant.

<br><br>

**(d) What is the coefficient estimate for the *Species* category Parkki? Interpret this coefficient.**

**Answer:**

The coefficient estimate of the Species category Parkki equals 79.34 .

This means that on average, the weight of Parkki is 79.34 g heavier than the weight of Bream, holding all the other variables constant.

<br><br>

# Question 3: Checking for Outliers and Multicollinearity [6 points]

**(a) Create a plot for the Cook's Distances. Using a threshold Cook's Distance of 1, identify the row numbers of any outliers.**

```{r}
cooks = cooks.distance(model1)
plot(cooks, type = 'h', ,lwd=3, ylab = "Cook's Distance")
abline(h=1, col = "red")

which(cooks>1)
```

**Answer:**

From the results above, the observation #30 is an outlier.

<br> <br>

**(b) Remove the outlier(s) from the data set and create a new model, called model2, using all predictors with *Weight* as the response. Display the summary of this model.**

```{r}


fish2 <- fish[-30,]

model2 <- lm(Weight ~ Species + Body.Height + Total.Length + Diagonal.Length +Height + Width, data = fish2)

summary(model2)
```

**(c) Display the VIF of each predictor for model2. Using a VIF threshold of max(10, 1/(1-**$R^2$) what conclusions can you draw?

```{r}
r.squared = summary(model2)$r.squared
threshold = max(10, 1/(1-r.squared))

cat("VIF thresold equals: ", threshold)
cat("\n\n")

vif(model2)
```

**Answers:**

From the calculations above, the VIF threshold equals 16.25583 .

The VIF values of all the predictors are all higher than this threshold. Therefore, this model has the issue of multicolinearity.

<br><br>

# Question 4: Checking Model Assumptions [6 points]

*Please use the cleaned data set, which have the outlier(s) removed, and model2 for answering the following questions.*

**(a) Create scatterplots of the standardized residuals of model2 versus each quantitative predictor. Does the linearity assumption appear to hold for all predictors?**

```{r}
library(MASS)
std.res <- stdres(model2)

par(mfrow = c(2,3))

plot(fish2$Body.Height, std.res, xlab = "Body.Height")
abline(h=0, col="red")

plot(fish2$Total.Length, std.res, xlab = "Total.Length")
abline(h=0, col="red")

plot(fish2$Diagonal.Length, std.res, xlab = "Diagonal.Length")
abline(h=0, col="red")

plot(fish2$Height, std.res, xlab = "Height")
abline(h=0, col="red")

plot(fish2$Width, std.res, xlab = "Width")
abline(h=0, col="red")
```

**Answer:**

The linearity assumption doesn't appear to hold for any predictors, because curvatures and U-shape patterns can be observed from each of the charts above.

<br><br>

**(b) Create a scatter plot of the standardized residuals of model2 versus the fitted values of model2. Does the constant variance assumption appear to hold? Do the errors appear uncorrelated?**

```{r}
fitted <- model2$fitted
plot(fitted, std.res)
abline(h = 0, col = "red")
```

**Answer:**

In the chart above, we can also see that the variance of the standardized residual is getting larger when the fitted value is larger than 200. Therefore, the constant variance assumption doesn't hold.

On the other side, the scatter plot is not exhibiting any obvious clusters of the standardized residuals. Therefore, the errors appears to be uncorrelated.

<br><br>

**(c) Create a histogram and normal QQ plot for the standardized residuals. What conclusions can you draw from these plots?**

```{r}
par(mfrow = c(1,2))

hist(std.res)
qqPlot(std.res)
```

**Answer:**

The histogram shows that the distribution of the standardized residuals is obviously right-skewed. In the QQ-plot, we can also observe some deviations from normal distributions at the tail.

Therefore, we can conclude that the normality assumption doesn't hold for model2. Some data transformations may be applied to improve the model's goodness of fit.

<br><br>

# Question 5: Partial F Test [6 points]

**(a) Build a third multiple linear regression model using the cleaned data set without the outlier(s), called model3, using only *Species* and *Total.Length* as predicting variables and *Weight* as the response. Display the summary table of the model3.**

```{r}

model3 <- lm(Weight ~ Species + Total.Length, data = fish2)
summary(model3)

```

**(b) Conduct a partial F-test comparing model3 with model2. What can you conclude using an** $\alpha$ level of 0.01?

```{r}

anova(model3, model2)
```

**Answer:**

The p-value of the partial F test equals 0.14, which is larger than 0.01 . This indicates that we can not reject the null hypothesis that the coefficients of the additional variables are all equal to zero, using an $\alpha$ level of 0.01.

In the words, Body.Height, Diagonal.Length, Height and Width are not adding additional explanatory power to the model, given Species and Total.Length .

<br><br>

# Question 6: Reduced Model Residual Analysis and Multicollinearity Test [7 points]

**(a) Conduct a multicollinearity test on model3. Comment on the multicollinearity in model3.**

```{r}
r.squared = summary(model3)$r.squared
threshold = max(10, 1/(1-r.squared))

cat("VIF thresold equals: ", threshold)
cat("\n\n")

vif(model3)
```

**Answer:**

From the calculations above, the VIF threshold equals 15.45466 .

Since the VIFs of both Species and Total.Length are smaller than this threshold, we can conclude that there is not significant multicolinearity issue in model3.

<br><br>

**(b) Conduct residual analysis for model3 (similar to Q4). Comment on each assumption and whether they hold.**

```{r}

std.res <- stdres(model3)
fitted <- model3$fitted

par(mfrow = c(2,2))
plot(fish2$Total.Length, std.res, xlab = "Total.Length")
abline(h=0, col="red")
plot(fitted, std.res, xlab = "Fitted Values")
abline(h=0, col="red")

hist(std.res)
qqPlot(std.res)

```

**Answer:**

The scatter plot between Total.Length and the standardized residuals and the scatter plot between the fitted values and the standardized residuals still display a U-shape pattern. This indicates that the linearity assumption doesn't hold.

In the scatter plot between the fitted values and the standardized residuals, we can see that the variance of standardized residuals gets larger when the fitted value is larger than 250. This means that the constant variance assumption do not hold.

The two scatter plots above do not show any obvious clusters of the standardized residuals. This indicates that the uncorrelation assumption holds.

The histogram is right-skewed. In the QQ plot, we can also observe some deviations from normal distribution at the tails. Therefore, the normality assumption doesn't hold.

In conclusion, the linearity assumption, the constant variance assumption and the normality assumption do not hold. We may apply some data transformations to improve the goodness of fit.

<br><br>

# Question 7: Transformation [9 pts]

**(a) Use model3 to find the optimal lambda, rounded to the nearest 0.5, for a Box-Cox transformation on model3. What transformation, if any, should be applied according to the lambda value? Please ensure you use model3**

```{r}

bc = boxCox(model3)
opt.lambda = bc$x[which.max(bc$y)]
cat("Optimal lambda:", round(opt.lambda/0.5)*0.5, end="\n")
```

**Answer:**

The calculations above shows that the optimal lambda (rounded to the nearest 0.5) equals 0.5 . Therefore, we need to transform Weight to its square root $Weight^{\frac{1}{2}}$.

<br><br>

**(b) Based on the results in (a), create model4 with the appropriate transformation. Display the summary.**

```{r}

model4 <- lm(sqrt(Weight) ~ Species + Total.Length, data = fish2)
summary(model4)
```

**(c) Perform Residual Analysis on model4. Comment on each assumption. Was the transformation successful/unsuccessful?**

```{r}

std.res <- stdres(model4)
fitted <- model4$fitted

par(mfrow = c(2,2))
plot(fish2$Total.Length, std.res, xlab = "Total.Length")
abline(h=0, col="red")
plot(fitted, std.res, xlab = "Fitted Values")
abline(h=0, col="red")

hist(std.res)
qqPlot(std.res)
```

**Answers:**

In the two scatter plots above, we can see that the standardized residuals are randomly distributed around zero, without any obvious patterns and clusters. Therefore, the linearity assumption, the constant variance assumption and the uncorrelation assumption hold.

In the histogram, we can see that the distribution of the standardized residuals is more symmetric than model3, but still slightly right-skewed. This can also be observed in the QQ plot - there are still some deviations from normal distribution at the tail, but in general, the standardized residuals now more comply with normal distribution.

In general, the transformation is successful, because it resolves the issue of non-linearity and non-constant variance. But some further investigations are still needed, in order to resolve the issue of non-normality.

<br><br>

# Question 8: Model Comparison [2 pts]

**(a) Using each model summary, compare and discuss the R-squared and Adjusted R-squared of model2, model3, and model4.**

```{r}
r.squared <- summary(model2)$r.squared
adj.r.squared <- summary(model2)$adj.r.squared
cat("Model2:  ", "R Squared = ", r.squared, "     ", "Adjusted R Squared = ", adj.r.squared, "\n")

r.squared <- summary(model3)$r.squared
adj.r.squared <- summary(model3)$adj.r.squared
cat("Model3:  ", "R Squared = ", r.squared, "     ", "Adjusted R Squared = ", adj.r.squared, "\n")

r.squared <- summary(model4)$r.squared
adj.r.squared <- summary(model4)$adj.r.squared
cat("Model4:  ", "R Squared = ", r.squared, "     ", "Adjusted R Squared = ", adj.r.squared, "\n")
```

**Answer:**

Model 4 has the highest R squared value and the highest adjusted R squared value. Therefore, it is easy to conclude that model 4 has the highest explanatory power among the three models. This high explanatory power can be attributed to the box-cox transformation.

Model 2 has slightly higher R squared value and adjusted R squared value than model 3. This is because model 2 includes more predictor variables. But the difference of explanatory power between model 2 and model 3 is so small that we can just ignore this difference.

<br><br>

# Question 9: Prediction [8 points]

**(a) Predict Weight for the last 10 rows of data (fishtest) using both model3 and model4. Compare and discuss the mean squared prediction error (MSPE) of both models.**

```{r}

pred <- predict(model3, newdata = fishtest)
MSPE <- mean((pred-fishtest$Weight)^2)
cat("MSPE of Model 3 = ", MSPE, "\n")

# need to take squared value of the prediction results of model 4
pred <- (predict(model4, newdata = fishtest))^2
MSPE <- mean((pred-fishtest$Weight)^2)
cat("MSPE of Model 4 = ", MSPE, "\n")

```

**Answer:**

Model 3's MSPE = 9392.25

Model 4's MSPE = 2442.998

The calculations above shows that model 4 has a higher prediction accuracy than model 3, since model 4 has a lower MSPE value.

<br><br>

**(b) Suppose you have found a Perch fish with a Body.Height of 28 cm, and a Total.Length of 32 cm. Using model4, predict the weight on this fish with a 90% prediction interval. Provide an interpretation of the prediction interval.**

```{r}

fishtest2 <- data.frame(Species = "Perch",  Body.Height = 28, Total.Length = 32)

# need to take squared value of the prediction results of model 4
(predict(model4, newdata = fishtest2, interval = "prediction", level = 0.9))^2
```

**Answer:**

The predicted value equals 461.95 g. The 90% prediction interval is (374.45 , 558.61)

This prediction interval means that for any **one** Perch fish with Total.Length = 32 cm , we are 90% confident that its weight will be between 374.45 g and 558.61 g.

(Body Height doesn't really matter since this variable is not included in the model.)
