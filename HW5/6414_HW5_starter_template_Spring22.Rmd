---
title: "Homework 5 Peer Assessment"
output:
  html_document: default
  pdf_document: default
date: "Spring Semester 2022"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(tinytex.verbose = TRUE)
```

## Background

Selected molecular descriptors from the Dragon chemoinformatics application were used to predict bioconcentration factors for 779 chemicals in order to evaluate QSAR (Quantitative Structure Activity Relationship). This dataset was obtained from the UCI machine learning repository.

The dataset consists of 779 observations of 10 attributes. Below is a brief description of each feature and the response variable (logBCF) in our dataset:

1.  *nHM* - number of heavy atoms (integer)
2.  *piPC09* - molecular multiple path count (numeric)
3.  *PCD* - difference between multiple path count and path count (numeric)
4.  *X2Av* - average valence connectivity (numeric)
5.  *MLOGP* - Moriguchi octanol-water partition coefficient (numeric)
6.  *ON1V* - overall modified Zagreb index by valence vertex degrees (numeric)
7.  *N.072* - Frequency of RCO-N\< / \>N-X=X fragments (integer)
8.  *B02[C-N]* - Presence/Absence of C-N atom pairs (binary)
9.  *F04[C-O]* - Frequency of C-O atom pairs (integer)
10. *logBCF* - Bioconcentration Factor in log units (numeric)

Note that all predictors with the exception of B02[C-N] are quantitative. For the purpose of this assignment, DO NOT CONVERT B02[C-N] to factor. Leave the data in its original format - numeric in R.

Please load the dataset "Bio_pred" and then split the dataset into a train and test set in a 80:20 ratio. Use the training set to build the models in Questions 1-6. Use the test set to help evaluate model performance in Question 7. Please make sure that you are using R version 3.6.X or above (i.e. version 4.X is also acceptable).

## Read Data

```{r}
#if (!require("CombMSC", character.only=TRUE)) {
#packageurl <- "https://cran.r-project.org/src/contrib/Archive/CombMSC/CombMSC_1.4.2.1.tar.gz"
#install.packages(packageurl, repos=NULL, type="source")
#library(CombMSC)
#}
```

```{r, message=F, warning=F}
# Clear variables in memory
rm(list=ls())

# Import the libraries
library(CombMSC)
library(boot)
library(leaps)
library(MASS)
library(glmnet)

# Ensure that the sampling type is correct
RNGkind(sample.kind="Rejection")

# Set a seed for reproducibility
set.seed(100)

# Read data
fullData = read.csv("E:/Data/Bio_pred.csv",header=TRUE)

# Split data for traIning and testing
testRows = sample(nrow(fullData),0.2*nrow(fullData))
testData = fullData[testRows, ]
trainData = fullData[-testRows, ]
```

Note: Use the training set to build the models in Questions 1-6. Use the test set to help evaluate model performance in Question 7.

## Question 1: Full Model

(a) Fit a multiple linear regression with the variable *logBCF* as the response and the other variables as predictors. Call it *model1*. Display the model summary.

```{r}

model1 <- lm(logBCF ~ ., data = trainData)
summary(model1)

```

(b) Which regression coefficients are significant at the 95% confidence level? At the 99% confidence level?

    **Response:**

    nHM, MLOGP, ON1V, B02[C-N] and F04[C-O]. are significant at the 95% confidence level.

    nHM, MLOGP and F04[C-O]. are significant at the 99% confidence level.

(c) What are the Mallow's Cp, AIC, and BIC criterion values for this model?

```{r, message=F, warning=F}
set.seed(100)

n = nrow(trainData)

#calculate CP
Cp(model1, S2=summary(model1)$sigma^2)

#Calculate AIC
AIC(model1, k=2)

#Calculate BIC
AIC(model1, k=log(n))

```

**Response:**

The Mallow's Cp equals 10. AIC equals 1497.477 . BIC equals 1546.274 .

(d) Build a new model on the training data with only the variables which coefficients were found to be statistically significant at the 99% confident level. Call it *model2*. Perform a Partial F-test to compare this new model with the full model (*model1*). Which one would you prefer? Is it good practice to select variables based on statistical significance of individual coefficients? Explain.

```{r}
set.seed(100)

model2 <- lm(logBCF ~ nHM + MLOGP + F04.C.O., data = trainData)
summary(model2)

anova(model1, model2)

```

**Response:**

I prefer model1, because:

1.  Both the R-squared and adjusted R-squared value in model1 are slightly higher than model2. This means that model1 can explain more variations in the response variable than model2.
2.  The partial F-test (ANOVA) shows a p-value equal to 0.00523 (smaller than 0.01) . This means that at least one of those variables excluded from model2 can add explanatory power to the model (at the confidence level of 99%) . Therefore, it is not appropriate to exclude all these variables from the model.

It is not a good practice to select variables based on statistical significance of individual coefficients. In the full model, the significant variables show high statistical significance, given other variables in the model. It can not be guaranteed that these variables are still significant after other variables are excluded from the model. For example, in model1, F04[C-O] is significant at the confidence level of 99%, but in model2, it is not significant at the level of 99% .

## Question 2: Full Model Search

(a) Compare all possible models using Mallow's Cp. How many models can be constructed using subsets/combinations drawn from the full set of variables? Display a table indicating the variables included in the best model of each size and the corresponding Mallow's Cp value.

Hint: You can use nbest parameter.

```{r, message=F, warning=F}
set.seed(100)

# calculate number of subsets
out <- leaps(trainData[,-c(10)], trainData[,10], method = "Cp")
nrow(out$which)

# best model of each size
out <- leaps(trainData[,-c(10)], trainData[,10], method = "Cp", nbest = 1)
cbind(as.matrix(out$which),out$Cp)
```

**Response:**

79 models can be constructed using subsets/combinations drawn from the full set of variables. The table showing the best model of each size and the corresponding Mallow's Cp value is displayed above.

(b) How many variables are in the model with the lowest Mallow's Cp value? Which variables are they? Fit this model and call it *model3*. Display the model summary.

```{r}
set.seed(100)

best_model <- which(out$Cp == min(out$Cp))
var_selection <- as.matrix(out$which)[best_model, ]

colnames(trainData[, -10])[var_selection]

model3 <- lm(logBCF ~ nHM + piPC09 + MLOGP + ON1V + B02.C.N. + F04.C.O.,
             data = trainData)
summary(model3)
```

**Response:**

Six variables are in the model with the lowest Mallow's Cp value. They are nHM, piPC09, MLOGP, ON1V, B02.C.N. and F04.C.O.

## Question 3: Stepwise Regression

(a) Perform backward stepwise regression using BIC. Allow the minimum model to be the model with only an intercept, and the full model to be *model1*. Display the model summary of your final model. Call it *model4*

```{r}
set.seed(100)

n = nrow(trainData)

minimum_model = lm(logBCF ~ 1, data = trainData)
model4 <- step(model1, scope=list(lower=minimum_model, 
                                  upper=model1), 
               direction="backward", k = log(n))
summary(model4)
```

(b) How many variables are in *model4*? Which regression coefficients are significant at the 99% confidence level?

    **Response:**

    Four variables are in model4. They are nHM, PiPC090, MLOGP and F04[C-O] . The coefficients of all the four variables are significant.

(c) Perform forward stepwise selection with AIC. Allow the minimum model to be the model with only an intercept, and the full model to be *model1*. Display the model summary of your final model. Call it *model5*. Do the variables included in *model5* differ from the variables in *model4*?

```{r}
set.seed(100)

model5 <- step(minimum_model, scope=list(lower=minimum_model, 
                                         upper=model1), 
               direction="forward", k = 2)
summary(model5)
```

**Response:**

The variables included in model5 is different from model4. B02[C-N] and ON1V are included in model5, but they are not included in model4.

(d) Compare the adjusted $R^2$, Mallow's Cp, AICs and BICs of the full model (*model1*), the model found in Question 2 (*model3*), and the model found using backward selection with BIC (*model4*). Which model is preferred based on these criteria and why?

```{r}
set.seed(100)

model_compare <- data.frame(
  Model = c(1,3,4),
  Adj.R.Squared = c(summary(model1)$adj.r.squared,
                    summary(model3)$adj.r.squared,
                    summary(model4)$adj.r.squared),
  Cp = c(Cp(model1, S2=summary(model1)$sigma^2),
         Cp(model3, S2=summary(model3)$sigma^2),
         Cp(model4, S2=summary(model4)$sigma^2)),
  AIC = c(AIC(model1, k=2),
          AIC(model3, k=2),
          AIC(model4, k=2)),
  BIC = c(AIC(model1, k=log(n)),
          AIC(model3, k=log(n)),
          AIC(model4, k=log(n)))
)

model_compare
```

**Response:**

Model3 and Model4 are preferred over model1.

Model3 is the best model according to AIC, while model4 is the best model according to BIC. At the meantime, model4 also has the lowest Cp value, which means that model4 has the lowest complexity and is the easiest one to interpret.

It makes sense to further compare the performance of model3 and model4 by using cross validation.

## Question 4: Ridge Regression

(a) Perform ridge regression on the training set. Use cv.glmnet() to find the lambda value that minimizes the cross-validation error using 10 fold CV.

```{r}
set.seed(100)

predictors = as.matrix(trainData[ , 1:9])
logBCF = trainData$logBCF

# perform ridge regression
ridge.cv <- cv.glmnet(predictors, logBCF, alpha=0, nfolds=10)

best_lambda <- ridge.cv$lambda.min
best_lambda
```

**Response:**

The lambda value that minimizes the cross-validation error using 10 fold CV equals 0.108775

(b) List the value of coefficients at the optimum lambda value.

```{r}
set.seed(100)

ridge <- glmnet(predictors, logBCF, alpha=0, nfolds=10)
coef(ridge, s=ridge.cv$lambda.min)
```

(c) How many variables were selected? Was this result expected? Explain.

    **Response:**

    All nine variables are selected. This result is expected because ridge regression should not be used as a variable selection method. Ridge regression will let the coefficients shrink towards zero, but it will not force the coefficients to equal to zero.

## Question 5: Lasso Regression

(a) Perform lasso regression on the training set.Use cv.glmnet() to find the lambda value that minimizes the cross-validation error using 10 fold CV.

```{r, message=F, warning=F}
set.seed(100)

# perform lasso regression
lasso.cv <- cv.glmnet(predictors, logBCF, alpha=1, nfolds=10)

best_lambda <- lasso.cv$lambda.min
best_lambda
```

**Response:**

The lambda value that minimizes the cross-validation error using 10 fold CV equals 0.007854436

(b) Plot the regression coefficient path.

```{r}
set.seed(100)

lasso = glmnet(predictors, logBCF, alpha = 1, nlambda=100)

plot(lasso,xvar="lambda", lwd=2)
abline(v=log(lasso.cv$lambda.min), col='black', lty=2)
```

(c) How many variables were selected? Which are they?

```{r}
set.seed(100)

coef(lasso, s=lasso.cv$lambda.min)

```

**Response:**

Eight variables are selected. They are: nHM, piPC09, PCD, MLOGP, ON1V, N.072, B02[C-N] and F04[C-O] .

## Question 6: Elastic Net

(a) Perform elastic net regression on the training set. Use cv.glmnet() to find the lambda value that minimizes the cross-validation error using 10 fold CV. Give equal weight to both penalties.

```{r}
set.seed(100)

elastic.cv <- cv.glmnet(predictors, logBCF, alpha=0.5, nfolds=10)

best_lambda <- elastic.cv$lambda.min
best_lambda
```

(b) List the coefficient values at the optimal lambda. How many variables were selected? How do these variables compare to those from Lasso in Question 5?

```{r}
set.seed(100)

elastic = glmnet(predictors, logBCF, alpha=0.5, nlambda = 100)
coef(elastic, s = elastic.cv$lambda.min)
```

**Response:**

Eight variables are selected. They are the same variables selected by LASSO regression.

## Question 7: Model comparison

(a) Predict *logBCF* for each of the rows in the test data using the full model, and the models found using backward stepwise regression with BIC, ridge regression, lasso regression, and elastic net. Display the first few predictions for each model.

```{r}
set.seed(100)

# create new predictor matrix
new_predictor <- as.matrix(testData[ , 1:9])

# full model
pred_full <- predict(model1, newdata = testData)
cat("Predictions for full model: \n\n")
head(pred_full)


# backward stepwise regression with BIC
pred_backward <- predict(model4, newdata = testData)
cat("Predictions for model found using backward stepwise regression with BIC: \n\n")
head(pred_backward)


# Ridge regression
pred_ridge <- predict(ridge, newx = new_predictor, s=ridge.cv$lambda.min)
cat("Predictions for model found using ridge regression : \n\n")
head(pred_ridge)

# Lasso & Elastic net
#Retrain the model
lasso.retrain <- lm(logBCF ~ nHM + piPC09 + PCD + MLOGP + ON1V + 
                             N.072 + B02.C.N. + F04.C.O. ,
                    data = trainData)
pred_lasso <- predict(lasso.retrain, newdata = testData)

cat("Predictions for model found using lasso regression : \n\n")
head(pred_lasso)

cat("Predictions for model found using elastic net (same as predictions by lasso regression) : \n\n")
pred_elastic <- pred_lasso
head(pred_elastic)
```

(b) Compare the predictions using mean squared prediction error. Which model performed the best?

```{r}
set.seed(100)

# full model
MSPE_full <- mean((pred_full - testData$logBCF)^2)
cat("MSPE of full model:", MSPE_full, "\n")

# backward stepwise regression with BIC
MSPE_backward <- mean((pred_backward - testData$logBCF)^2)
cat("MSPE of model found using backward stepwise regression with BICl: ",MSPE_backward, "\n")

# ridge regression
MSPE_ridge <- mean((pred_ridge - testData$logBCF)^2)
cat("MSPE of ridge regression: ",MSPE_ridge, "\n")

# lasso regression
MSPE_lasso <- mean((pred_lasso - testData$logBCF)^2)
cat("MSPE of lasso regression: ", MSPE_lasso, "\n")

# elastic net
MSPE_elastic <- mean((pred_elastic - testData$logBCF)^2)
cat("MSPE of lasso regression: ",MSPE_elastic,"\n")
```

**Response:**

From the results above, the model found using backward stepwise regression with BIC performs the best.

(c) Provide a table listing each method described in Question 7a and the variables selected by each method (see Lesson 5.8 for an example). Which variables were selected consistently?

|          | Backward Stepwise | Ridge | Lasso | Elastic Net |
|----------|-------------------|-------|-------|-------------|
| nHM      | x                 | x     | x     | x           |
| piPC09   | x                 | x     | x     | x           |
| PCD      |                   | x     | x     | x           |
| X2AV     |                   | x     |       |             |
| MLOGP    | x                 | x     | x     | x           |
| ON1V     |                   | x     | x     | x           |
| N.072    |                   | x     | x     | x           |
| B02.C.N. |                   | x     | x     | x           |
| F04.C.O. | x                 | x     | x     | x           |

**Response:**

nHM, piPc09, MLOGP and F04.C.O. are selected consistently.
